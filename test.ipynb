{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6edf4d0b8614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.pipeline import EntityLinker\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "#import matplotlib.colors as mcolors\n",
    "\n",
    "nytimes = pd.read_csv('include/nytimes.csv', sep=',', nrows=10)\n",
    "newest_doc = nytimes['lead_paragraph']\n",
    "\n",
    "print(newest_doc.head(5))\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "# My list of stop words.\n",
    "stop_list = [\"Mrs.\", \"Ms.\", \"say\", \"The\", \"Â´s\", \"Mr.\"]\n",
    "\n",
    "# Updates spaCy's default stop words list with my additional words.\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "\n",
    "# Iterates over the words in the stop words list and resets the \"is_stop\" flag.\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True\n",
    "\n",
    "\n",
    "def lemmatizer(doc):\n",
    "    # This takes in a doc of tokens from the NER and lemmatizes them.\n",
    "    # Pronouns (like \"I\" and \"you\" get lemmatized to '-PRON-', so I'm removing those.\n",
    "    doc = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    doc = u' '.join(doc)\n",
    "    return nlp.make_doc(doc)\n",
    "\n",
    "\n",
    "def remove_stopwords(doc):\n",
    "    # This will remove stopwords and punctuation.\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# The add_pipe function appends our functions to the default pipeline.\n",
    "nlp.add_pipe(lemmatizer, name='lemmatizer', after='ner')\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)\n",
    "\n",
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for doc in tqdm(newest_doc):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(doc)\n",
    "    doc_list.append(pr)\n",
    "\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=words,\n",
    "                                            num_topics=10,\n",
    "                                            random_state=2,\n",
    "                                            update_every=1,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics(num_topics=10))\n",
    "\n",
    "\n",
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "class APIError(Exception):\n",
    "    def __init__(self, status):\n",
    "        self.status = status\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"APIError: status={}\".format(self.status)\n",
    "\n",
    "\n",
    "base_url = \"http://api.dbpedia-spotlight.org/en/annotate\"\n",
    "\n",
    "# Parameters\n",
    "# 'text' - text to be annotated\n",
    "# 'confidence' -   confidence score for linking\n",
    "params = {\"text\": \"My name is Sundar. I am currently doing Master's in Artificial Intelligence at NUS. I love Natural Language Processing.\", \"confidence\": 0.35}\n",
    "\n",
    "headers = {'accept': 'text/html'}\n",
    "\n",
    "res = requests.get(base_url, params=params, headers=headers)\n",
    "if res.status_code != 200:\n",
    "    raise APIError(res.status_code)\n",
    "\n",
    "display(HTML(res.text))\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# with open('test.txt', 'r', newline='\\n') as csvfile:\n",
    "#     #spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "#        #                     quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "#      reader = csv.DictReader(csvfile)\n",
    "#      for row in reader:\n",
    "#         print(row['box1'], row['box2'])\n",
    "# # with open('test.txt', 'r') as myfile:\n",
    "# #     data = myfile.read()\n",
    "# #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
